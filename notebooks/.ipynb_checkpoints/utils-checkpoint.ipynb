{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ntpath\n",
    "import pickle as pkl\n",
    "import xlrd\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file_in_folder(folder_path):\n",
    "\n",
    "    files = glob.glob(folder_path + '*')\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):    \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    return x.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(dt_str):\n",
    "    dt_str = dt_str.strip()\n",
    "    dtobj = datetime.strptime(dt_str, '%m/%d/%Y %I:%M:%S %p')\n",
    "    return dtobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags_time(txt):\n",
    "    '''\n",
    "    @{PIPoint=SCTM:22GTWY_E403:FALE22E23SP.PNT; Value=60; Timestamp=12/30/2017 11:48:05 PM}\n",
    "    '''\n",
    "    pi_point, val, time  = None, None, None\n",
    "    delimiter = ';'\n",
    "    sub_delimiter = '='\n",
    "    \n",
    "    txt = txt[txt.find('{')+1:txt.find('}')]    \n",
    "    parsed_vals = txt.split(';')\n",
    "    \n",
    "    if len(parsed_vals) >= 3:\n",
    "        pi_point = parsed_vals[0].split(sub_delimiter)[1]\n",
    "    \n",
    "        if pi_point is not None:\n",
    "            values = parsed_vals[1].split(sub_delimiter)\n",
    "            if len(values) >= 2:\n",
    "                val = values[1]\n",
    "                if is_number(val):\n",
    "                    val = float(val)\n",
    "                else:\n",
    "                    val = val\n",
    "            else:\n",
    "                val = None\n",
    "\n",
    "            time_vals = parsed_vals[2].split(sub_delimiter)\n",
    "            if len(time_vals) >= 2:\n",
    "                time = time_vals[1]\n",
    "                time = get_time(time)\n",
    "            else:\n",
    "                return None, None, None\n",
    "\n",
    "    if pi_point is not None:\n",
    "        pi_point = pi_point.replace('SCTM:', '')\n",
    "    \n",
    "    return pi_point, val, time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22GTWY_E403:FALE22E23SP.PNT Error 2017-12-30 23:48:05\n"
     ]
    }
   ],
   "source": [
    "txt = '@{PIPoint=SCTM:22GTWY_E403:FALE22E23SP.PNT; Value=60; Timestamp=12/30/2017 11:48:05 PM}'\n",
    "txt = '@{PIPoint=SCTM:22GTWY_E403:FALE22E23SP.PNT; Value=Error; Timestamp=12/30/2017 11:48:05 PM}'\n",
    "a,b,c = parse_tags_time(txt)\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(txt):\n",
    "    '''\n",
    "    @{PIPoint=SCTM:22GTWY_E403:FALE22E23SP.PNT; Value=60; Timestamp=12/30/2017 11:48:05 PM}\n",
    "    '''\n",
    "    pi_point, val, time  = None, None, None\n",
    "    delimiter = ';'\n",
    "    sub_delimiter = '='\n",
    "    \n",
    "    txt = txt[txt.find('{')+1:txt.find('}')]    \n",
    "    parsed_vals = txt.split(';')\n",
    "    \n",
    "    if len(parsed_vals) >= 3:\n",
    "        pi_point = parsed_vals[0].split(sub_delimiter)[1]\n",
    "    \n",
    "        if pi_point is not None:\n",
    "            values = parsed_vals[1].split(sub_delimiter)\n",
    "            if len(values) >= 2:\n",
    "                val = values[1]\n",
    "                if is_number(val):\n",
    "                    val = float(val)\n",
    "                else:\n",
    "                    val = val\n",
    "            else:\n",
    "                val = None\n",
    "\n",
    "            time_vals = parsed_vals[2].split(sub_delimiter)\n",
    "            if len(time_vals) >= 2:\n",
    "                time = time_vals[1]\n",
    "                time = get_time(time)\n",
    "            else:\n",
    "                return None, None, None\n",
    "\n",
    "    if pi_point is not None:\n",
    "        pi_point = pi_point.replace('SCTM:', '')\n",
    "    \n",
    "    return pi_point, val, time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubstringFinder(string1, string2):\n",
    "    '''\n",
    "    Code from https://stackoverflow.com/questions/18715688/find-common-substring-between-two-strings    \n",
    "    '''\n",
    "    answer = \"\"\n",
    "    len1, len2 = len(string1), len(string2)\n",
    "    for i in range(len1):\n",
    "        match = \"\"\n",
    "        for j in range(len2):\n",
    "            if (i + j < len1 and string1[i + j] == string2[j]):\n",
    "                match += string2[j]\n",
    "            else:\n",
    "                if (len(match) > len(answer)): answer = match\n",
    "                match = \"\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_withfeature(input_path, print_debug = False):\n",
    "    \n",
    "    df_features = {}\n",
    "    \n",
    "    if os.path.isdir(input_path):\n",
    "        input_files = [f for f in listdir(input_path) if (isfile(join(input_path, f))) and ((f.endswith('.pkl')) or (f.endswith('.csv')))]\n",
    "    elif os.path.isfile(input_path):\n",
    "        input_files = input_path\n",
    "    \n",
    "    if print_debug:\n",
    "        print('Number of files found in %s is %d ' % (input_path, len(input_files)))\n",
    "    \n",
    "    for input_file in input_files:\n",
    "        # feature,_ = os.path.splitext(input_file)\n",
    "        input_file = input_path + input_file  \n",
    "        \n",
    "        with open(input_file, 'rb') as f:\n",
    "            df = pkl.load(f)\n",
    "            unq_features = np.unique(df['feature'])\n",
    "            # print(input_file, unq_features)\n",
    "            if len(unq_features) > 0:\n",
    "                if len(unq_features) > 1:\n",
    "                    if print_debug:\n",
    "                        print('There are %d features in file %s' % (len(unq_features), input_file))\n",
    "                    continue            \n",
    "                feature = unq_features[0]            \n",
    "                df_features[feature]= df\n",
    "    \n",
    "    if print_debug:\n",
    "        print('Number of features extracted from %d files is %d ' % (len(input_files), len(df_features)))\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path, print_debug = False):\n",
    "    \n",
    "    df_features = {}\n",
    "    \n",
    "    if os.path.isdir(input_path):\n",
    "        input_files = [f for f in listdir(input_path) if (isfile(join(input_path, f))) and ((f.endswith('.pkl')) or (f.endswith('.csv')))]\n",
    "    elif os.path.isfile(input_path):\n",
    "        input_files = input_path\n",
    "    \n",
    "    if print_debug:\n",
    "        print('Number of files found in %s is %d ' % (input_path, len(input_files)))\n",
    "    \n",
    "    for input_file in input_files:\n",
    "        \n",
    "        feature,_ = os.path.splitext(input_file)\n",
    "        input_file = input_path + input_file  \n",
    "        feature = feature.replace('-', ':')\n",
    "                \n",
    "        with open(input_file, 'rb') as f:\n",
    "            df = pkl.load(f)\n",
    "            df_features[feature] = df\n",
    "                \n",
    "    if print_debug:\n",
    "        print('Number of features extracted from %d files is %d ' % (len(input_files), len(df_features)))\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats_df(df_features):\n",
    "\n",
    "    df_stats = pd.DataFrame(columns=['feature', 'total_count', 'missing_val_count', 'min_date', 'max_date', 'max_val', 'min_val', 'variance', 'std', 'mean_val', 'median_val', 'kurt', 'skew'])\n",
    "    idx = 0\n",
    "\n",
    "    for feature, df in df_features.items():\n",
    "        df_stats.loc[idx] = [feature, len(df), len(df)-df['val'].count(), df['datetime'].min(), df['datetime'].max(), df['val'].max(), df['val'].min(), df['val'].var(), df['val'].std(), df['val'].mean(), df['val'].median(), df['val'].kurt(), df['val'].skew()]\n",
    "        idx += 1\n",
    "        \n",
    "    # Get the percentage missing values\n",
    "    df_stats['perc_missing'] = df_stats['missing_val_count']/df_stats['total_count'] * 100\n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features with substantial missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_features(df_features, value_col, max_missing_vals_pcnt):\n",
    "    features = list(df_features.keys())\n",
    "    for feature in features:\n",
    "        df = df_features[feature]\n",
    "        num_rows = len(df)\n",
    "        count = df[value_col].count()\n",
    "        percent_missing = (num_rows-count)/num_rows * 100\n",
    "        # print(feature, percent_missing)\n",
    "        if percent_missing > max_missing_vals_pcnt:\n",
    "            del df_features[feature]\n",
    "            # print('-------------------')\n",
    "            \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dateime <--> Integer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seconds_after(current_date, base_date):\n",
    "    \n",
    "    base_ts = time.mktime(base_date.timetuple()) # Converting to Unix timestamp\n",
    "    current_ts = time.mktime(current_date.timetuple())\n",
    "    time_diff = round((current_ts - base_ts))\n",
    "    \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minutes_after(current_date, base_date):\n",
    "        \n",
    "    base_ts = time.mktime(base_date.timetuple()) # Converting to Unix timestamp\n",
    "    current_ts = time.mktime(current_date.timetuple())\n",
    "    time_diff = round((current_ts - base_ts) / 60.0) + 1\n",
    "    \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours_after(current_date, base_date):\n",
    "    \n",
    "    base_ts = time.mktime(base_date.timetuple()) # Converting to Unix timestamp\n",
    "    current_ts = time.mktime(current_date.timetuple())\n",
    "    time_diff = round((current_ts - base_ts) / 60.0 / 60.0) + 1\n",
    "    \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_after(current_date, base_date):\n",
    "    \n",
    "    base_ts = time.mktime(base_date.timetuple()) # Converting to Unix timestamp\n",
    "    current_ts = time.mktime(current_date.timetuple())\n",
    "    time_diff = round((current_ts - base_ts) / 60.0 / 60.0 / 24) + 1\n",
    "    \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_months_after(current_date, base_date):    \n",
    "    time_diff = ((current_date.year - base_date.year) * 12) + current_date.month - base_date.month + 1    \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years_after(current_date, base_date):    \n",
    "    time_diff = (current_date.year - base_date.year) + 1 \n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_from_minutes(time_in_mins, base_date):    \n",
    "    new_date = base_date + timedelta(minutes = time_in_mins)        \n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_granulairty_function(granularity, current_date, base_date):\n",
    "    \n",
    "    if granularity == 'sec':\n",
    "        return get_seconds_after(current_date, base_date)\n",
    "    elif granularity == 'min':\n",
    "        return get_minutes_after(current_date, base_date)\n",
    "    elif granularity == 'hr':\n",
    "        return get_hours_after(current_date, base_date)\n",
    "    elif granularity == 'day':\n",
    "        return get_days_after(current_date, base_date)\n",
    "    elif granularity == 'mon':\n",
    "        return get_months_after(current_date, base_date)\n",
    "    elif granularity == 'yr':\n",
    "        return get_years_after(current_date, base_date)\n",
    "    \n",
    "    return get_minutes_after(current_date, base_date) # Default return function is for minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation for duplicate timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_data_dup_timestamps(df_features, \n",
    "                            feature_set, \n",
    "                            time_granularity, \n",
    "                            time_col, \n",
    "                            value_col, \n",
    "                            time_gran_col, \n",
    "                            base_date):\n",
    "    \n",
    "    df_features_e = {}\n",
    "    \n",
    "    if len(feature_set) == 0:\n",
    "        feature_set = list(df_features.keys())\n",
    "        \n",
    "    for feature in feature_set:\n",
    "        \n",
    "        print(feature + ' -- Started ', end='')\n",
    "        \n",
    "        df = df_features[feature].copy()\n",
    "        len_df_before = len(df)\n",
    "        \n",
    "        df.drop(columns=['feature'], inplace=True) # Drop the feature column as its redundant\n",
    "\n",
    "        # Drop duplicates\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df.sort_values(by=[time_col], inplace=True, ascending=True)\n",
    "\n",
    "        # Compute the granularity\n",
    "        df[time_gran_col] = df[time_col].apply(lambda x:get_granulairty_function(time_granularity, x, base_date))\n",
    "\n",
    "        # Average if there are more readings within the same granularity level\n",
    "        df = df.dropna(subset=[value_col])\n",
    "        df = df.drop(columns=[time_col])\n",
    "\n",
    "        # Convert the value column to numeric to help in aggregation of duplicate timestamps\n",
    "        df[value_col] = pd.to_numeric(df[value_col])\n",
    "\n",
    "        # AGGREGATE the duplicate timestamps - take the mean\n",
    "        df_g = df[[time_gran_col, value_col]].groupby(time_gran_col).mean()\n",
    "        df = df_g.reset_index(level=0, inplace=False)\n",
    "        len_df_after = len(df)\n",
    "        \n",
    "        print('Ended - Aggregated %d rows to %d rows' % (len_df_before, len_df_after))\n",
    "        \n",
    "        \n",
    "        df_features_e[feature] = df \n",
    "        \n",
    "        \n",
    "    return df_features_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing down files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feature_dict(dir_path, df_features, remove_existing=True):\n",
    "    \n",
    "    if remove_existing:\n",
    "        # Remove the files from the directory\n",
    "        remove_file_in_folder(dir_path)\n",
    "\n",
    "    # Write the pickle files to the folder\n",
    "    for feature in df_features.keys():\n",
    "        df = df_features[feature].copy()\n",
    "        \n",
    "        fname = feature.replace(':', '-')\n",
    "\n",
    "        pkl_file = dir_path + fname + '.pkl'\n",
    "        # print('Writing to file ', pkl_file, df.shape, '[', feature, ']')\n",
    "        \n",
    "        with open(pkl_file, 'wb') as f:\n",
    "            pkl.dump(df, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Master Dataframe for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def generate_master_df(time_granularity,\n",
    "                       time_gran_col,\n",
    "                       base_date,\n",
    "                       end_date,\n",
    "                       print_debug = False):\n",
    "    '''\n",
    "    Generates a master dataframe\n",
    "    Dataframe will have an integer column that denotes x minutes have passed after the base_date\n",
    "    granulaity - can take one of the following - 'sec' (seconds), min ' (minutes), 'hr' (hour), \n",
    "                'day' (day), 'mon' (month), 'yr' (year)\n",
    "    base_date = date of reference since which the unit of time is computed\n",
    "    '''\n",
    "    \n",
    "    if print_debug:\n",
    "        print('Granularity is', time_granularity, '\\tStart Date = ', base_date, '\\tEnd Date = ', end_date)\n",
    "            \n",
    "    max_td = get_granulairty_function(time_granularity, end_date, base_date)\n",
    "    \n",
    "    df_master = pd.DataFrame(columns=[time_gran_col])    \n",
    "    df_master[time_gran_col] = [i for i in range(1, max_td+1)]\n",
    "    \n",
    "    if print_debug:\n",
    "        print('Shape of the master dataframe is ', df_master.shape, 'with columns ', df_master.columns.values)\n",
    "    \n",
    "    return df_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_val(val, min_val, max_val):\n",
    "    if val is not None:\n",
    "        return (val-min_val)/(max_val-min_val + 1e-7)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcl_divmul(val, div_by, mul_by):\n",
    "    val = math.floor(val/div_by)\n",
    "    val = val * mul_by\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming files inside sub directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Frequency of read time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def freq_intervals(df_features, feature_set, bin_size, min_occurence, percentage=False, plot=False):\n",
    "    \n",
    "    prev_time_gran_col = 'prev_' + time_gran_col\n",
    "    prev_value_col = 'prev_' + value_col\n",
    "    diff_in_time = 'diff_in_time'\n",
    "    # total_plots = 0\n",
    "\n",
    "    if len(feature_set) == 0:\n",
    "        feature_set = list(df_features.keys())\n",
    "                \n",
    "    if plot:        \n",
    "        total_plots = len(feature_set)\n",
    "        cols_plot = 3\n",
    "        rows_plot = math.ceil(total_plots/cols_plot)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [cols_plot * 10, rows_plot * 7] # Size of the plots\n",
    "        plt.figure()\n",
    "        \n",
    "        xlabel = 'Time Granularity - mins/hr/day'\n",
    "        ylabel = 'Frequency'\n",
    "        \n",
    "            \n",
    "    idx = 1\n",
    "    for feature in feature_set:\n",
    "    \n",
    "        df = df_features[feature].copy()\n",
    "        \n",
    "        print(feature + ' -- Started ', end='')\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "\n",
    "        # for i in range(1, len(df)):\n",
    "        #    df.loc[i, prev_time_gran_col] = df.loc[i-1, time_gran_col]  \n",
    "        \n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "        \n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "        \n",
    "        # print(df.head())\n",
    "        \n",
    "        # Binning the dataset\n",
    "        diff = df[diff_in_time].copy()\n",
    "        \n",
    "        buckets = diff.apply(lambda x: math.floor(x/bin_size) * bin_size)\n",
    "        buckets = list(buckets)\n",
    "        bucket_len = len(buckets)\n",
    "        counter = Counter(buckets)\n",
    "\n",
    "        # The percentages in frequency_dict wont add upto 100 since only\n",
    "        # values more than 1 are added to the new dictionary - look at the \n",
    "        # if statement below\n",
    "        frequency_dict = {}\n",
    "        for k in counter:\n",
    "            v = counter[k]\n",
    "            if v >= min_occurence:\n",
    "                if percentage:\n",
    "                    v = v / bucket_len * 100.0\n",
    "                    ylabel += ' (%)'\n",
    "                k = str(k * bin_size + 1) + '-' + str( (k+1) * bin_size)\n",
    "                frequency_dict[k] = v\n",
    "                \n",
    "        if plot:\n",
    "            x = list(frequency_dict.keys())\n",
    "            x = list(map(str, x))\n",
    "            y = list(frequency_dict.values())\n",
    "                        \n",
    "            plt.subplot(rows_plot, cols_plot, idx)\n",
    "            # plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(feature)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.bar(x,y)\n",
    "            \n",
    "            for i, v in enumerate(y):\n",
    "                v = round(v, 2)\n",
    "                plt.text(i-.25, v + .25, str(v), color='blue', fontweight='bold')\n",
    "            \n",
    "        print('Ended')\n",
    "            \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the missing intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def missing_data_intervals(df_features, feature_set, min_interval_span, plot=False):\n",
    "    \n",
    "    prev_time_gran_col = 'prev_' + time_gran_col\n",
    "    prev_value_col = 'prev_' + value_col\n",
    "    diff_in_time = 'diff_in_time'\n",
    "    # total_plots = 0\n",
    "\n",
    "    if len(feature_set) == 0:\n",
    "        feature_set = list(df_features.keys())\n",
    "                \n",
    "    if plot:        \n",
    "        total_plots = len(feature_set)\n",
    "        cols_plot = 3\n",
    "        rows_plot = math.ceil(total_plots/cols_plot)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [cols_plot * 10, rows_plot * 7] # Size of the plots\n",
    "        plt.figure()\n",
    "        \n",
    "        xlabel = 'Time Granularity - mins/hr/day'\n",
    "        ylabel = 'Frequency'\n",
    "        \n",
    "            \n",
    "    idx = 1\n",
    "    for feature in feature_set:\n",
    "    \n",
    "        df = df_features[feature].copy()\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "        \n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "        \n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "\n",
    "        # Find only the dataset which is at least min_interval_span size\n",
    "        df = df_features_agg[feature].copy()\n",
    "        \n",
    "        print(feature + ' -- Started ', end='')\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "\n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "\n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "\n",
    "        # Find only the dataset which is at least min_interval_span size\n",
    "        df = df.loc[df[diff_in_time] >= min_occurence]\n",
    "\n",
    "        df[prev_time_col] = df[prev_time_gran_col].apply(lambda x: get_time_from_minutes(x, base_date))\n",
    "        df[prev_time_col] = df[prev_time_col].apply(lambda x: x.date())\n",
    "                     \n",
    "                \n",
    "        if plot:\n",
    "            x = df[prev_time_col]\n",
    "            x = list(map(str, x))\n",
    "            y = df[diff_in_time]\n",
    "                        \n",
    "            plt.subplot(rows_plot, cols_plot, idx)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(feature)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.bar(x,y)\n",
    "            \n",
    "            for i, v in enumerate(y):\n",
    "                v = round(v, 2)\n",
    "                plt.text(i-.25, v + .25, str(v), color='blue', fontweight='bold')\n",
    "            \n",
    "        print('Ended')\n",
    "            \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alarm Tag types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def freq_intervals(df_features, feature_set, bin_size, min_occurence, percentage=False, plot=False):\n",
    "    \n",
    "    prev_time_gran_col = 'prev_' + time_gran_col\n",
    "    prev_value_col = 'prev_' + value_col\n",
    "    diff_in_time = 'diff_in_time'\n",
    "    time_gran_col = 'datetime_gran'\n",
    "    # total_plots = 0\n",
    "\n",
    "    if len(feature_set) == 0:\n",
    "        feature_set = list(df_features.keys())\n",
    "                \n",
    "    if plot:        \n",
    "        total_plots = len(feature_set)\n",
    "        cols_plot = 3\n",
    "        rows_plot = math.ceil(total_plots/cols_plot)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [cols_plot * 10, rows_plot * 7] # Size of the plots\n",
    "        plt.figure()\n",
    "        \n",
    "        xlabel = 'Time Granularity - mins/hr/day'\n",
    "        ylabel = 'Frequency'\n",
    "        \n",
    "            \n",
    "    idx = 1\n",
    "    for feature in feature_set:\n",
    "    \n",
    "        df = df_features[feature].copy()\n",
    "        \n",
    "        print(feature + ' -- Started ', end='')\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "\n",
    "        # for i in range(1, len(df)):\n",
    "        #    df.loc[i, prev_time_gran_col] = df.loc[i-1, time_gran_col]  \n",
    "        \n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "        \n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "        \n",
    "        # print(df.head())\n",
    "        \n",
    "        # Binning the dataset\n",
    "        diff = df[diff_in_time].copy()\n",
    "        \n",
    "        buckets = diff.apply(lambda x: math.floor(x/bin_size) * bin_size)\n",
    "        buckets = list(buckets)\n",
    "        bucket_len = len(buckets)\n",
    "        counter = Counter(buckets)\n",
    "\n",
    "        # The percentages in frequency_dict wont add upto 100 since only\n",
    "        # values more than 1 are added to the new dictionary - look at the \n",
    "        # if statement below\n",
    "        frequency_dict = {}\n",
    "        for k in counter:\n",
    "            v = counter[k]\n",
    "            if v >= min_occurence:\n",
    "                if percentage:\n",
    "                    v = v / bucket_len * 100.0\n",
    "                    ylabel += ' (%)'\n",
    "                k = str(k * bin_size + 1) + '-' + str( (k+1) * bin_size)\n",
    "                frequency_dict[k] = v\n",
    "                \n",
    "        if plot:\n",
    "            x = list(frequency_dict.keys())\n",
    "            x = list(map(str, x))\n",
    "            y = list(frequency_dict.values())\n",
    "                        \n",
    "            plt.subplot(rows_plot, cols_plot, idx)\n",
    "            # plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(feature)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.bar(x,y)\n",
    "            \n",
    "            for i, v in enumerate(y):\n",
    "                v = round(v, 2)\n",
    "                plt.text(i-.25, v + .25, str(v), color='blue', fontweight='bold')\n",
    "            \n",
    "        print('Ended')\n",
    "            \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22GTWY_E403:FALE22E23SP.PNT Error 2017-12-30 23:48:05\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from utils import get_time_from_minutes\n",
    "\n",
    "def missing_data_intervals(df_features, feature_set, min_interval_span, plot=False):\n",
    "    \n",
    "    prev_time_gran_col = 'prev_' + time_gran_col\n",
    "    prev_value_col = 'prev_' + value_col\n",
    "    prev_time_col = 'prev_' + 'time'\n",
    "    diff_in_time = 'diff_in_time'\n",
    "    time_gran_col = 'datetime_gran'\n",
    "    # total_plots = 0\n",
    "\n",
    "    if len(feature_set) == 0:\n",
    "        feature_set = list(df_features.keys())\n",
    "                \n",
    "    if plot:        \n",
    "        total_plots = len(feature_set)\n",
    "        cols_plot = 3\n",
    "        rows_plot = math.ceil(total_plots/cols_plot)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [cols_plot * 10, rows_plot * 7] # Size of the plots\n",
    "        plt.figure()\n",
    "        \n",
    "        xlabel = 'Time Granularity - mins/hr/day'\n",
    "        ylabel = 'Frequency'\n",
    "        \n",
    "            \n",
    "    idx = 1\n",
    "    for feature in feature_set:\n",
    "    \n",
    "        df = df_features[feature].copy()\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "        \n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "        \n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "\n",
    "        # Find only the dataset which is at least min_interval_span size\n",
    "        df = df_features_agg[feature].copy()\n",
    "        \n",
    "        print(feature + ' -- Started ', end='')\n",
    "\n",
    "        df[prev_value_col] = df[value_col]\n",
    "        df[prev_time_gran_col] = df[time_gran_col]\n",
    "\n",
    "        df[prev_time_gran_col] = df[time_gran_col].shift(1)\n",
    "        df.dropna(subset=[prev_time_gran_col], inplace=True)\n",
    "\n",
    "        # Get the difference values\n",
    "        df[diff_in_time] = df[time_gran_col] - df[prev_time_gran_col]\n",
    "\n",
    "        # Find only the dataset which is at least min_interval_span size\n",
    "        df = df.loc[df[diff_in_time] >= min_interval_span]\n",
    "\n",
    "        df[prev_time_col] = df[prev_time_gran_col].apply(lambda x: get_time_from_minutes(x, base_date))\n",
    "        df[prev_time_col] = df[prev_time_col].apply(lambda x: x.date())\n",
    "                     \n",
    "                \n",
    "        if plot:\n",
    "            x = df[prev_time_col]\n",
    "            x = list(map(str, x))\n",
    "            y = df[diff_in_time]/min_interval_span\n",
    "            \n",
    "            if len(x) > 0:\n",
    "                        \n",
    "                plt.subplot(rows_plot, cols_plot, idx)\n",
    "                plt.ylabel(ylabel)\n",
    "                plt.title(feature)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.bar(x,y)\n",
    "                \n",
    "                idx += 1\n",
    "\n",
    "                for i, v in enumerate(y):\n",
    "                    v = round(v, 2)\n",
    "                    plt.text(i-.25, v + .25, str(v), color='blue', fontweight='bold')\n",
    "            \n",
    "        print('Ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_categories():\n",
    "\n",
    "    tag_categ = {}\n",
    "    tag_categ['vibration'] = [\n",
    "        '05GTWY_BN06:XT61B10.PNT',\n",
    "        '05GTWY_BN06:XT61B11.PNT',\n",
    "        '05GTWY_BN06:XT61B12.PNT',\n",
    "        '05GTWY_BN06:XT61B13.PNT',\n",
    "        '05GTWY_BN06:XT61B17.PNT',\n",
    "        '05GTWY_BN06:XT61B18.PNT',\n",
    "        '05GTWY_BN06:XT61B19.PNT',\n",
    "        '05GTWY_BN06:XT61B20.PNT',\n",
    "        '05GTWY_BN06:ZT61B14.PNT',\n",
    "        '05GTWY_BN06:ZT61B15.PNT'\n",
    "    ]\n",
    "\n",
    "    tag_categ['temperature'] = [\n",
    "        'TT61B01.PV',\n",
    "        'TT61B02.PV',\n",
    "        'TT61B03.PV',\n",
    "        'TT61B04.PV',\n",
    "        'TT61B05.PV',\n",
    "        'TT61B06.PV',\n",
    "        'TC63109E.AV',\n",
    "        'TT63109.PV',\n",
    "        '07DATASCRCP1:AI07003.PNT'\n",
    "    ]\n",
    "\n",
    "    tag_categ['pressure'] = [\n",
    "        'PIE61608.PV',\n",
    "        'PIE61B00.PV',\n",
    "        'PIE61B22.PV',\n",
    "        'PIE61B23.PV',\n",
    "        'PIE63113.PV',\n",
    "        'PT61A98.PV',\n",
    "        'PT61B00.PV',\n",
    "        'PT63103.PV',\n",
    "        'PC63112E.AV',\n",
    "        'PT63112.PV'\n",
    "    ]\n",
    "\n",
    "    tag_categ['level'] = [\n",
    "        'LT63114.PV',\n",
    "        'PC61A98.AV',\n",
    "        'PC63112.AV',\n",
    "        'TC63109.AV',\n",
    "        'F61221VP',\n",
    "        'T6150.PV',\n",
    "        'T6151.PV',\n",
    "        'T6152.PV',\n",
    "        'T6153.PV'\n",
    "    ]\n",
    "\n",
    "    tag_categ['alarm'] = [\n",
    "        'FA61A99.PV',\n",
    "        'LA63114.PV',\n",
    "        'LAL63114.PV',\n",
    "        'PA61B00.PV',\n",
    "        'PA61B223.PV',\n",
    "        'PA63110.PV',\n",
    "        'PA63112.PV',\n",
    "        'PA63113.PV',\n",
    "        'PAH61A98.PV',\n",
    "        'PAL61A98.PV',\n",
    "        'PDA61B21.PV',\n",
    "        'TA61B47.PV',\n",
    "        'TA63109.PV',\n",
    "        'TAE61B47.PV',\n",
    "        'XA61B34.PV',\n",
    "        'XA61B58.PV',\n",
    "        'XAE61B34.PV'\n",
    "    ]\n",
    "\n",
    "    tag_categ['flow'] = [\n",
    "        'FT61A99.PV',\n",
    "        'F61221'\n",
    "    ]\n",
    "\n",
    "    tag_categ['status'] = [\n",
    "        'P6302BDI.PV',\n",
    "        'P6302BSD.PV'\n",
    "    ]\n",
    "\n",
    "    tag_categ['setpoint'] = [\n",
    "        '05GTWY_BN06:XT61B16.PNT',\n",
    "        'PC61A98E.AV'\n",
    "    ]\n",
    "    \n",
    "    return tag_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(tag_type):\n",
    "    tag_categ = tag_categories()\n",
    "    if tag_type in tag_categ.keys():\n",
    "        return tag_categ[tag_type]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_type(tag):\n",
    "    tag_categ = tag_categories()\n",
    "    tag_types = []\n",
    "    for tag_type, tag_names in tag_categ.items():\n",
    "        if tag in tag_names:\n",
    "            tag_types.append(tag_type)\n",
    "        \n",
    "    return tag_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "shell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
