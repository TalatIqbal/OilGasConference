{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling based on the tags available in Data string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ntpath\n",
    "import pickle as pkl\n",
    "import xlrd\n",
    "import time\n",
    "import string\n",
    "from os.path import basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = '../data/raw/'\n",
    "PROC_DATA_DIR = '../data/processed/'\n",
    "INT_DATA_DIR = '../data/interim/P2253/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Files - PUMP 2253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_2253_DIRLIST = {'2017':'P-2253 2017', '2018':'P-2253 2018'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    return x.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files in P-2253 2017 is 44\n",
      "No of files in P-2253 2018 is 50\n"
     ]
    }
   ],
   "source": [
    "data_files_17 = {} # Contains absolute paths (values) of files mapped using their respective basenames (keys)\n",
    "data_files_18 = {}\n",
    "\n",
    "# 2017\n",
    "DIR = DATA_2253_DIRLIST['2017']\n",
    "dir_files = os.listdir(RAW_DATA_DIR + DIR)\n",
    "print('No of files in %s is %d' % (DIR, len(dir_files)))\n",
    "for dir_file in dir_files:\n",
    "    file_base = dir_file.replace('2017.csv','')\n",
    "    file_base = remove_punctuation(file_base)\n",
    "    data_files_17[file_base] = RAW_DATA_DIR + DIR + '/' + dir_file\n",
    "      \n",
    "# 2018\n",
    "DIR = DATA_2253_DIRLIST['2018']\n",
    "dir_files = os.listdir(RAW_DATA_DIR + DIR)\n",
    "print('No of files in %s is %d' % (DIR, len(dir_files)))\n",
    "for dir_file in dir_files:\n",
    "    file_base = dir_file.replace('2018.csv','')\n",
    "    file_base = remove_punctuation(file_base)\n",
    "    data_files_18[file_base] = RAW_DATA_DIR + DIR + '/' + dir_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['XI22F26Y', '22E24PNT', 'XI22F30X', 'TI22F12', 'TC22F38SPT', 'FC22E04', 'ZI22F27', '22E24SP', 'XI22F30Y', 'TI22F19AB', 'TI22F13', 'FXC22E22out', 'XI22F26X', 'FC22E22', 'PI22E05', 'II22E47', 'FXC22E22spt', 'FX22E22SPF', '22E23SP', 'TI22F11', 'TC22F38out', 'TI22F16AB', 'FX22E22CMP', 'XI22F25X', 'TI22F18AB', 'TC22F38', 'ZI22F28', 'TI22F15B', 'SI22F23', 'FC22E22SPT', 'XI22F25Y', 'TI22F14', '22F32', 'FC22E04SPT', 'FXC22E22', 'XI22F29Y', 'Flowbalance', 'FC22E22OUT', 'TI22F17AB', 'FC22E04OUT', 'FQI22E22', 'PI22F31', 'XI22F29X', 'TI22F15A'])\n",
      "../data/raw/P-2253 2017/XI22F26Y2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Use Data to find the common files\n",
    "print(data_files_17.keys())\n",
    "print(data_files_17['XI22F26Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(dt_str):\n",
    "    dt_str = dt_str.strip()\n",
    "    dtobj = datetime.strptime(dt_str, '%m/%d/%Y %I:%M:%S %p')\n",
    "    return dtobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(txt):\n",
    "    '''\n",
    "    @{PIPoint=SCTM:22GTWY_E403:FALE22E23SP.PNT; Value=60; Timestamp=12/30/2017 11:48:05 PM}\n",
    "    '''\n",
    "    pi_point, val, time  = None, None, None\n",
    "    delimiter = ';'\n",
    "    sub_delimiter = '='\n",
    "    \n",
    "    txt = txt[txt.find('{')+1:txt.find('}')]    \n",
    "    parsed_vals = txt.split(';')\n",
    "    \n",
    "    if len(parsed_vals) >= 3:\n",
    "        pi_point = parsed_vals[0].split(sub_delimiter)[1]\n",
    "    \n",
    "        values = parsed_vals[1].split(sub_delimiter)\n",
    "        if len(values) >= 2:\n",
    "            val = values[1]\n",
    "            if is_number(val):\n",
    "                val = float(val)\n",
    "            else:\n",
    "                val = None\n",
    "        else:\n",
    "            val = None\n",
    "\n",
    "        time_vals = parsed_vals[2].split(sub_delimiter)\n",
    "        if len(time_vals) >= 2:\n",
    "            time = time_vals[1]\n",
    "            time = get_time(time)\n",
    "        else:\n",
    "            return None, None, None\n",
    "    \n",
    "    return pi_point, val, time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_val(val, min_val, max_val):\n",
    "    if val is not None:\n",
    "        return (val-min_val)/(max_val-min_val + 1e-7)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minutes_after(current_date, base_date):\n",
    "    \n",
    "    base_ts = time.mktime(base_date.timetuple()) # Converting to Unix timestamp\n",
    "    current_ts = time.mktime(current_date.timetuple())\n",
    "    time_diff_min = round((current_ts - base_ts) / 60.0)\n",
    "    \n",
    "    return time_diff_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "base_date = datetime.strptime('2017-01-01 00:00:01', fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Tag Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files in P-2253 2018 is 50\n",
      "\n",
      " SCTM:22GTWY_E402:PALE22F32SP.PNT \t F32SP2018.csv\n",
      "SCTM:22GTWY_E402:PALE22F32SP.PNT \t E23SP2018.csv\n",
      "\n",
      "===============\n",
      "\n",
      " SCTM:22GTWY_E402:PALE22F32SP.PNT \t F322018.csv\n",
      "SCTM:22GTWY_E402:PALE22F32SP.PNT \t E23SP2018.csv\n",
      "SCTM:22GTWY_E402:PALE22F32SP.PNT \t F32SP2018.csv\n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "data_18 = {} # Contains mapping between tags and data\n",
    "tag_files_18 = {} # Contains mapping between tags and the file names\n",
    "files_tag_18 = {} # Contains the mapping between file names and tags\n",
    "DIR = DATA_2253_DIRLIST['2018']\n",
    "data_files = data_files_18\n",
    "\n",
    "data = {}\n",
    "tag_files = {}\n",
    "files_tag = {}\n",
    "\n",
    "lst_files = []\n",
    "\n",
    "dir_files = os.listdir(RAW_DATA_DIR + DIR)\n",
    "print('No of files in %s is %d' % (DIR, len(dir_files)))\n",
    "\n",
    "for current_file in dir_files:\n",
    "    \n",
    "    current_file = RAW_DATA_DIR + DIR + '/' + current_file    \n",
    "    current_base = basename(current_file)\n",
    "    \n",
    "    df = pd.read_csv(current_file, header=None)\n",
    "    \n",
    "    df['pi_point'], df['val'], df['read_time'] = zip(*df[0].map(parse))\n",
    "       \n",
    "    df = df[df['pi_point'] != None]\n",
    "    df = df[df['read_time'].notnull()]\n",
    "    \n",
    "    '''\n",
    "    min_val = df['val'].min()\n",
    "    max_val = df['val'].max()\n",
    "        \n",
    "    df['time_in_mins'] = df['read_time'].apply(lambda x:get_minutes_after(x, base_date))\n",
    "    df['scaled_val'] = df['val'].apply(lambda x:scale_val(x, min_val, max_val))\n",
    "    df.sort_values(['time_in_mins'], inplace=True, ascending=True)\n",
    "    '''\n",
    "    \n",
    "    # Find the tags that are duplicates - same tag found in 2 different files\n",
    "    tag = df.at[2, 'pi_point']\n",
    "    if tag in tag_files.keys():\n",
    "        print('\\n', tag, '\\t', current_base)\n",
    "        for k,v in files_tag.items():\n",
    "            if v == tag:\n",
    "                print(v, '\\t', k)\n",
    "        print('\\n===============')\n",
    "                \n",
    "    \n",
    "    tag_files[tag] = current_base\n",
    "    files_tag[current_base] = tag\n",
    "    \n",
    "    data[tag] = df\n",
    "    \n",
    "data_18 = data\n",
    "tag_files_18 = tag_files\n",
    "files_tag_18 = files_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of data from the files and mapping them using the tags found in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files in P-2253 2018 is 50\n",
      "Processing  IX22E472018.csv\n",
      "Skipping  E23SP2018.csv\n",
      "Processing  FQI22E222018.csv\n",
      "Processing  PI22F312018.csv\n",
      "Processing  XI22F29X2018.csv\n",
      "Processing  TI22F15A2018.csv\n",
      "Processing  Flow_balance2018.csv\n",
      "Processing  FXC22E222018.csv\n",
      "Processing  XI22F29Y2018.csv\n",
      "Processing  FX22E22SP2018.csv\n",
      "Processing  FC22E22out2018.csv\n",
      "Processing  FC22E04out2018.csv\n",
      "Processing  TI22F18B2018.csv\n",
      "Processing  TI22F19B2018.csv\n",
      "Processing  TI22F15B2018.csv\n",
      "Processing  FC22E22spt2018.csv\n",
      "Processing  E232018.csv\n",
      "Processing  XI22F25Y2018.csv\n",
      "Processing  TI22F142018.csv\n",
      "Processing  XI22F25X2018.csv\n",
      "Processing  TI22F18A2018.csv\n",
      "Processing  TI22F19A2018.csv\n",
      "Processing  E24SP2018.csv\n",
      "Processing  ZI22F282018.csv\n",
      "Skipping  F32SP2018.csv\n",
      "Processing  FX22E22CMP2018.csv\n",
      "Processing  TC22F3812018.csv\n",
      "Processing  PI22E052018.csv\n",
      "Processing  II22E472018.csv\n",
      "Processing  FC22E04SP2018.csv\n",
      "Processing  FXC22E22SPT2018.csv\n",
      "Processing  TI22F16A2018.csv\n",
      "Processing  TI22F17A2018.csv\n",
      "Processing  TI22F112018.csv\n",
      "Processing  TC22F381SPT2018.csv\n",
      "Skipping  F322018.csv\n",
      "Processing  FC22E042018.csv\n",
      "Processing  ZI22F272018.csv\n",
      "Processing  TC22F381out2018.csv\n",
      "Processing  XI22F30Y2018.csv\n",
      "Processing  TI22F132018.csv\n",
      "Processing  FXC22E22out2018.csv\n",
      "Processing  XI22F26X2018.csv\n",
      "Processing  E242018.csv\n",
      "Processing  FC22E222018.csv\n",
      "Processing  XI22F26Y2018.csv\n",
      "Processing  TI22F16B2018.csv\n",
      "Processing  TI22F17B2018.csv\n",
      "Processing  XI22F30X2018.csv\n",
      "Processing  TI22F122018.csv\n"
     ]
    }
   ],
   "source": [
    "data_18 = {} # Contains mapping between tags and data\n",
    "tag_files_18 = {} # Contains mapping between tags and the file names\n",
    "files_tag_18 = {} # Contains the mapping between file names and tags\n",
    "DIR = DATA_2253_DIRLIST['2018']\n",
    "data_files = data_files_18\n",
    "\n",
    "# Have a list for files whose tags are still not certain\n",
    "excluded_list = ['F32SP2018.csv','E23SP2018.csv','F322018.csv']\n",
    "excluded_list = [x.lower() for x in excluded_list]\n",
    "\n",
    "data = {}\n",
    "tag_files = {}\n",
    "files_tag = {}\n",
    "\n",
    "lst_files = []\n",
    "\n",
    "dir_files = os.listdir(RAW_DATA_DIR + DIR)\n",
    "print('No of files in %s is %d' % (DIR, len(dir_files)))\n",
    "\n",
    "for current_file in dir_files:\n",
    "    \n",
    "    # If the current file a part of exluded list, skip the file\n",
    "    if current_file.lower() in excluded_list:\n",
    "        print('Skipping ', current_file)\n",
    "        continue\n",
    "    \n",
    "    print('Processing ', current_file)\n",
    "    current_file = RAW_DATA_DIR + DIR + '/' + current_file\n",
    "    current_base = basename(current_file)\n",
    "    \n",
    "    df = pd.read_csv(current_file, header=None)\n",
    "    \n",
    "    df['pi_point'], df['val'], df['read_time'] = zip(*df[0].map(parse))\n",
    "       \n",
    "    df = df[df['pi_point'] != None]\n",
    "    df = df[df['read_time'].notnull()]\n",
    "    \n",
    "    '''\n",
    "    min_val = df['val'].min()\n",
    "    max_val = df['val'].max()\n",
    "        \n",
    "    df['time_in_mins'] = df['read_time'].apply(lambda x:get_minutes_after(x, base_date))\n",
    "    df['scaled_val'] = df['val'].apply(lambda x:scale_val(x, min_val, max_val))\n",
    "    df.sort_values(['time_in_mins'], inplace=True, ascending=True)\n",
    "    '''\n",
    "    \n",
    "    tag = df.at[2, 'pi_point']\n",
    "    tag_files[tag] = current_base\n",
    "    files_tag[current_base] = tag\n",
    "    \n",
    "    data[tag] = df\n",
    "    \n",
    "data_18 = data\n",
    "tag_files_18 = tag_files\n",
    "files_tag_18 = files_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files in P-2253 2017 is 44\n",
      "Processing  XI22F26Y2017.csv\n",
      "Processing  22E24PNT2017.csv\n",
      "Processing  XI22F30X2017.csv\n",
      "Processing  TI22F122017.csv\n",
      "Processing  TC22F38SPT2017.csv\n",
      "Processing  FC22E042017.csv\n",
      "Processing  ZI22F272017.csv\n",
      "Processing  22E24SP2017.csv\n",
      "Processing  XI22F30Y2017.csv\n",
      "Processing  TI22F19A_B2017.csv\n",
      "Processing  TI22F132017.csv\n",
      "Processing  FXC22E22out2017.csv\n",
      "Processing  XI22F26X2017.csv\n",
      "Processing  FC22E222017.csv\n",
      "Processing  PI22E052017.csv\n",
      "Processing  II22E472017.csv\n",
      "Processing  FXC22E22spt2017.csv\n",
      "Processing  FX22E22SPF2017.csv\n",
      "Processing  22E23SP2017.csv\n",
      "Processing  TI22F112017.csv\n",
      "Processing  TC22F38out2017.csv\n",
      "Processing  TI22F16A_B2017.csv\n",
      "Processing  FX22E22CMP2017.csv\n",
      "Processing  XI22F25X2017.csv\n",
      "Processing  TI22F18A_B2017.csv\n",
      "Processing  TC22F382017.csv\n",
      "Processing  ZI22F282017.csv\n",
      "Processing  TI22F15B2017.csv\n",
      "Processing  SI22F232017.csv\n",
      "Processing  FC22E22SPT2017.csv\n",
      "Processing  XI22F25Y2017.csv\n",
      "Processing  TI22F142017.csv\n",
      "Processing  22F322017.csv\n",
      "Processing  FC22E04SPT2017.csv\n",
      "Processing  FXC22E222017.csv\n",
      "Processing  XI22F29Y2017.csv\n",
      "Processing  Flowbalance2017.csv\n",
      "Processing  FC22E22OUT2017.csv\n",
      "Processing  TI22F17A_B2017.csv\n",
      "Processing  FC22E04OUT2017.csv\n",
      "Processing  FQI22E222017.csv\n",
      "Processing  PI22F312017.csv\n",
      "Processing  XI22F29X2017.csv\n",
      "Processing  TI22F15A2017.csv\n"
     ]
    }
   ],
   "source": [
    "data_17 = {} # Contains mapping between tags and data\n",
    "tag_files_17 = {} # Contains mapping between tags and the file names\n",
    "files_tag_17 = {} # Contains the mapping between file names and tags\n",
    "DIR = DATA_2253_DIRLIST['2017']\n",
    "data_files = data_files_17\n",
    "\n",
    "# Have a list for files whose tags are still not certain\n",
    "excluded_list = []\n",
    "excluded_list = [x.lower() for x in excluded_list]\n",
    "\n",
    "data = {}\n",
    "tag_files = {}\n",
    "files_tag = {}\n",
    "\n",
    "lst_files = []\n",
    "\n",
    "dir_files = os.listdir(RAW_DATA_DIR + DIR)\n",
    "print('No of files in %s is %d' % (DIR, len(dir_files)))\n",
    "\n",
    "for current_file in dir_files:\n",
    "    \n",
    "    # If the current file a part of exluded list, skip the file\n",
    "    if current_file.lower() in excluded_list:\n",
    "        print('Skipping ', current_file)\n",
    "        continue\n",
    "    \n",
    "    print('Processing ', current_file)\n",
    "    current_file = RAW_DATA_DIR + DIR + '/' + current_file\n",
    "    current_base = basename(current_file)\n",
    "    \n",
    "    df = pd.read_csv(current_file, header=None)\n",
    "    \n",
    "    df['pi_point'], df['val'], df['read_time'] = zip(*df[0].map(parse))\n",
    "       \n",
    "    df = df[df['pi_point'] != None]\n",
    "    df = df[df['read_time'].notnull()]\n",
    "    \n",
    "    '''\n",
    "    min_val = df['val'].min()\n",
    "    max_val = df['val'].max()\n",
    "        \n",
    "    df['time_in_mins'] = df['read_time'].apply(lambda x:get_minutes_after(x, base_date))\n",
    "    df['scaled_val'] = df['val'].apply(lambda x:scale_val(x, min_val, max_val))\n",
    "    df.sort_values(['time_in_mins'], inplace=True, ascending=True)\n",
    "    '''\n",
    "    \n",
    "    tag = df.at[2, 'pi_point']\n",
    "    tag_files[tag] = current_base\n",
    "    files_tag[current_base] = tag\n",
    "    \n",
    "    data[tag] = df\n",
    "    \n",
    "data_17 = data\n",
    "tag_files_17 = tag_files\n",
    "files_tag_17 = files_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 tags =  47\n",
      "2017 tags =  44\n",
      "common tags =  41\n"
     ]
    }
   ],
   "source": [
    "print('2018 tags = ', len(set(tag_files_18.keys())))\n",
    "print('2017 tags = ', len(set(tag_files_17.keys())))\n",
    "common_tags = set(tag_files_18.keys()).intersection(set(tag_files_17.keys()))\n",
    "print('common tags = ', len(common_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Dataframes from both years and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file  ../data/interim/P2253/common/PDIE22E24.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/HCU_P2253_Flow_Balance.Cal.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FXC22E22.OUT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TC22F38.MEAS.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F13.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F26X.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/PDALE22E24SP.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F30Y.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E04.SPT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/PI22E05.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E22.OUT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F12.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FX22E22CMP.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TC22F38.OUT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F15B.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F26Y.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E04.MEAS.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F19A.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F30X.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/II22E47.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F17A.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FQI22E22.OUT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F25X.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E22.SPT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FXC22E22.MEAS.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F11.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/ZI22F27.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FX22E22SPF.MEAS.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FXC22E22.SPT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E22.MEAS.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TC22F38.SPT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F15A.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F18A.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F29Y.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/PI22F31.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F14.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/ZI22F28.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/TI22F16A.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F25Y.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/XI22F29X.PNT.pkl\n",
      "Writing to file  ../data/interim/P2253/common/FC22E04.OUT.pkl\n"
     ]
    }
   ],
   "source": [
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "base_date = datetime.strptime('2017-01-01 00:00:01', fmt)### Combining Dataframes from both years and Normalization\n",
    "\n",
    "for tag in common_tags:\n",
    "    \n",
    "    df = data_17[tag].append(data_18[tag])\n",
    "    \n",
    "    # Normalization\n",
    "    min_val = df['val'].min()\n",
    "    max_val = df['val'].max()\n",
    "        \n",
    "    df['time_in_mins'] = df['read_time'].apply(lambda x:get_minutes_after(x, base_date))\n",
    "    df['scaled_val'] = df['val'].apply(lambda x:scale_val(x, min_val, max_val))\n",
    "    df.sort_values(['time_in_mins'], inplace=True, ascending=True)\n",
    "    \n",
    "    '''\n",
    "    print(data_17[tag].shape)\n",
    "    print(data_18[tag].shape)\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    '''\n",
    "    \n",
    "    pkl_file = INT_DATA_DIR + 'common/' + tag.split(':')[2] + '.pkl'\n",
    "    print('Writing to file ', pkl_file)\n",
    "    # print('===================')\n",
    "    \n",
    "    with open(pkl_file, 'wb') as f:\n",
    "        pkl.dump(df, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "shell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
